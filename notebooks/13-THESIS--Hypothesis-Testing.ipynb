{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45c9e933",
   "metadata": {},
   "source": [
    "### Imported Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a63b75b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from arch import arch_model #GARCH Models\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.stats\n",
    "from statistics import NormalDist\n",
    "from scipy.stats import t\n",
    "from scipy.stats import f\n",
    "from datetime import datetime\n",
    "from scipy.stats import shapiro #Shapiro Test\n",
    "from scipy import stats #t-test\n",
    "import statsmodels.api as sm #Ljung-Box Test\n",
    "from scipy.stats import chi2\n",
    "import statistics as stat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdd68b5",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "I will only be using PH data for the initial test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49c32b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2006-01-01' #yyyy-mm-dd\n",
    "end_date = '2021-01-01'\n",
    "\n",
    "\n",
    "#Philippines\n",
    "PH = pd.read_csv('https://raw.githubusercontent.com/raphaelyt/thesis199.11/main/data/2006-2021/PSEi.csv')\n",
    "PH['Date'] = pd.to_datetime(PH['Date'])\n",
    "PH = PH.rename(columns={'Price': 'Close'})\n",
    "PH = PH.replace(',','', regex=True)\n",
    "PH['Close'] = PH['Close'].astype(float, errors = 'raise')\n",
    "mask = (PH['Date'] >= start_date) & (PH['Date'] <= end_date)\n",
    "PH = PH.loc[mask]\n",
    "PH = PH.set_index('Date')\n",
    "PH = PH.sort_index(axis=0, ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801495ee",
   "metadata": {},
   "source": [
    "### Logarithmic Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9e67cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_returns(df,d):\n",
    "    '''\n",
    "    The function obtains the log returns of the asset shifted d days\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The data frame contains data of a chosen stock index \n",
    "        (Stock index must be arrange in DESCENDING ORDER by DATE)\n",
    "    d : int\n",
    "        The dth day being forecast\n",
    "        (Assumed to be 1 for most cases)\n",
    "        \n",
    "    RETURNS\n",
    "    -------\n",
    "    df : pandas.DataFrame\n",
    "        The data frame returns an updated data frame containing the\n",
    "        'Returns' column\n",
    "    '''\n",
    "    df['Previous'] = df['Close'].shift(-d)\n",
    "    df['Returns'] = np.log(df['Close']/df['Previous'])*100\n",
    "    return df.dropna()\n",
    "\n",
    "PH = get_returns(PH, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a86f65",
   "metadata": {},
   "source": [
    "# Delta-Normal Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be37b57",
   "metadata": {},
   "source": [
    "### Estimating VaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bb503af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PH: 2.18134\n",
      "PH: 3.08511\n",
      "PH: 4.93201\n"
     ]
    }
   ],
   "source": [
    "def get_VaR_DN(df, alpha):\n",
    "    '''\n",
    "    The function returns the d-day VaR of the asset\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The data frame contains data of a chosen stock index \n",
    "        (Stock index must be arrange in DESCENDING ORDER by DATE)\n",
    "    alpha : float \n",
    "        The level of significance of the VaR\n",
    "        (Assumes a value in between 0 and 1)\n",
    "        \n",
    "    RETURNS\n",
    "    -------\n",
    "    VaR : float\n",
    "        The float returned is the VaR for the d-day with a \n",
    "        (1-alpha)% VaR\n",
    "    '''\n",
    "    sigma = np.std(df['Returns'])\n",
    "    VaR = sigma*NormalDist().inv_cdf(1-alpha)\n",
    "    return round(VaR, 5)\n",
    "\n",
    "alpha = 0.05\n",
    "print(f'PH: {get_VaR_DN(PH, alpha)}')\n",
    "\n",
    "alpha = 0.01\n",
    "print(f'PH: {get_VaR_DN(PH, alpha)}')\n",
    "\n",
    "alpha = 0.0001\n",
    "print(f'PH: {get_VaR_DN(PH, alpha)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c517153",
   "metadata": {},
   "source": [
    "### Fixed Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaaf3c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_size = 365\n",
    "alpha = 0.05\n",
    "name = 'FW-DN-5%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21bcfc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_window(df, test_size, alpha):\n",
    "    '''\n",
    "    The function returns the (1-alpha)% d-day VaR of the asset staring at\n",
    "    time t = test size to the present using a fixed time window of size test_size\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The data frame contains data of a chosen stock index \n",
    "        (Stock index must be arrange in DESCENDING ORDER by DATE)\n",
    "    test_size : int\n",
    "        The test size is used to create a time window to estimate VaR\n",
    "        using smaller time periods\n",
    "        (The test size cannot be greater than the total number of entries\n",
    "        of the data frame)\n",
    "    alpha : float \n",
    "        The level of significance of the VaR\n",
    "        (Assumes a value in between 0 and 1)\n",
    "\n",
    "    RETURNS\n",
    "    -------\n",
    "    VaR_df : pandas.DataFrame\n",
    "        The data frame contains the estimated (1-alpha)% d-day VaR from \n",
    "        time t = test size to current using the fixed time window \n",
    "    '''\n",
    "    VaR_lst = []\n",
    "    date_df = df[:test_size].reset_index()\n",
    "    date_df = date_df.filter(['Date'])\n",
    "    for i in range(test_size):\n",
    "        temp_df = df[test_size-i:-(i+1)]\n",
    "        temp_VaR = get_VaR_DN(temp_df, alpha)\n",
    "        VaR_lst.append(temp_VaR)\n",
    "    VaR_lst = VaR_lst[::-1]\n",
    "    VaR_df = pd.DataFrame(VaR_lst, columns = ['Forecasted VaR'])\n",
    "    VaR_df = pd.merge(date_df, VaR_df, left_index = True, right_index = True)\n",
    "    VaR_df = VaR_df.set_index('Date')\n",
    "    VaR_df.index = pd.to_datetime(VaR_df.index, utc = None)\n",
    "    return VaR_df\n",
    "\n",
    "#Standard 365 test subjects\n",
    "\n",
    "\n",
    "PH_fw_dn = fixed_window(PH, prediction_size,alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ebb9f",
   "metadata": {},
   "source": [
    "### Rolling Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97e9b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_size = 365\n",
    "alpha = 0.05\n",
    "name = 'RW-DN-5%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "622071f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(df, test_size, alpha):\n",
    "    '''\n",
    "    The function returns the (1-alpha)% d-day VaR of the asset staring at\n",
    "    time t = test size to the present using a smaller time window to begin\n",
    "    and gradually increasing the size\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The data frame contains data of a chosen stock index \n",
    "        (Stock index must be arrange in DESCENDING ORDER by DATE)\n",
    "    test_size : int\n",
    "        The test size is used to create a time window to estimate VaR\n",
    "        using smaller time periods\n",
    "        (The test size cannot be greater than the total number of entries\n",
    "        of the data frame)\n",
    "    alpha : float \n",
    "        The level of significance of the VaR\n",
    "        (Assumes a value in between 0 and 1)\n",
    "\n",
    "    RETURNS\n",
    "    -------\n",
    "    VaR_df : pandas.DataFrame\n",
    "        The data frame contains the estimated (1-alpha)% d-day VaR from \n",
    "        time t = test size to current using the rolling time window \n",
    "    '''\n",
    "    VaR_lst = []\n",
    "    date_df = df[:test_size].reset_index()\n",
    "    date_df = date_df.filter(['Date'])\n",
    "    for i in range(test_size):\n",
    "        temp_df = df[test_size-i:]\n",
    "        temp_VaR = get_VaR_DN(temp_df, alpha)\n",
    "        VaR_lst.append(temp_VaR)\n",
    "    VaR_lst = VaR_lst[::-1]\n",
    "    VaR_df = pd.DataFrame(VaR_lst, columns = ['Forecasted VaR'])\n",
    "    VaR_df = pd.merge(date_df, VaR_df, left_index = True, right_index = True)\n",
    "    VaR_df = VaR_df.set_index('Date')\n",
    "    return VaR_df\n",
    "\n",
    "#Standard\n",
    "PH_rw_dn = rolling_window(PH, prediction_size, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2589d34",
   "metadata": {},
   "source": [
    "# GARCH Model Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6cb331",
   "metadata": {},
   "source": [
    "### Determining the Order of the GARCH Model (Standard is GARCH(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34471438",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "decay = 0.992\n",
    "vol = 'GARCH'\n",
    "p = 1\n",
    "q = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e0db894",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def fit_params(df, vol,  p = None, q = None):\n",
    "    returns = df['Returns'].dropna(axis = 0)\n",
    "    model = arch_model(returns, vol = vol, p = p, q = q, rescale = False)\n",
    "    fit = model.fit(disp = 'off')\n",
    "    params = fit.params\n",
    "    resid = fit.resid\n",
    "    condv = fit.conditional_volatility\n",
    "    return fit, params, resid, condv\n",
    "\n",
    "PH_fit_g, PH_params_g, PH_resid_g, PH_condv_g = fit_params(PH, vol, p, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e72419f",
   "metadata": {},
   "source": [
    "### Verifying Order Determination (Ljung-Box Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac067fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        lb_stat      lb_pvalue\n",
      "20  1242.925736  4.891077e-251\n"
     ]
    }
   ],
   "source": [
    "#Philippines\n",
    "print(sm.stats.acorr_ljungbox((PH_resid_g**2).dropna(), lags=[20], return_df=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07453da",
   "metadata": {},
   "source": [
    "### Estimating VaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a04b9663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_VaR_GARCH(df, alpha, sigma, dist):\n",
    "    if dist == 'normal':\n",
    "        VaR = sigma*NormalDist().inv_cdf(1-alpha) #t.ppf(1-alpha, len(df['Returns'])-1 )\n",
    "    elif dist == 't':\n",
    "        VaR = sigma*t.ppf(1-alpha, len(df['Returns'])-1 )\n",
    "    return VaR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0308db",
   "metadata": {},
   "source": [
    "### Fixed Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66d42027",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 365\n",
    "alpha = 0.05\n",
    "vol = 'GARCH'\n",
    "d = 1\n",
    "dist = 'normal'\n",
    "p = 1\n",
    "q = 1\n",
    "name = 'FW-G-5%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9ec3af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_window(df, test_size, d, alpha, p, q, vol, dist):\n",
    "    df = df[::-1]\n",
    "    returns = df['Returns'].dropna(axis = 0)\n",
    "    sigma_lst = []\n",
    "    date_df = df.iloc[:test_size]\n",
    "    date_df = date_df.reset_index()\n",
    "    date_df = date_df.filter(['Date'])\n",
    "    for i in range(test_size):\n",
    "        temp_df = returns[i:-(test_size-i)]\n",
    "        model = arch_model(temp_df, p=p, q=q, vol = vol, rescale=None)\n",
    "        fit = model.fit(disp='off')\n",
    "        pred = fit.forecast(horizon=d, reindex = False)\n",
    "        sigma_lst.append(np.sqrt(pred.variance.values[-1,:][0]))\n",
    "    sigma_df = pd.Series(sigma_lst, index=returns.index[-test_size:])\n",
    "    VaR_df = get_VaR_GARCH(df, alpha, sigma_df, dist)\n",
    "    return sigma_df, VaR_df\n",
    "\n",
    "\n",
    "PH_sigma_df_g, PH_fw_df_g = fixed_window(PH, test_size,d, alpha,p,q, vol, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ea18b9",
   "metadata": {},
   "source": [
    "### Rolling Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e294125",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 365\n",
    "alpha = 0.05\n",
    "d = 1\n",
    "p = 1\n",
    "q = 1\n",
    "dist = 'normal'\n",
    "vol = 'GARCH'\n",
    "name = 'RW-G-5%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc105d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(df, test_size, d, alpha, p, q, vol, dist):\n",
    "    df = df[::-1]\n",
    "    returns = df['Returns'].dropna(axis = 0)\n",
    "    sigma_lst = []\n",
    "    date_df = df.iloc[:test_size]\n",
    "    date_df = date_df.reset_index()\n",
    "    date_df = date_df.filter(['Date'])\n",
    "    for i in range(test_size):\n",
    "        temp_df = returns[:-(test_size-i)]\n",
    "        model = arch_model(temp_df, p=p, q=q, vol = vol , rescale=None)\n",
    "        fit = model.fit(disp='off')\n",
    "        pred = fit.forecast(horizon=d, reindex = False)\n",
    "        sigma_lst.append(np.sqrt(pred.variance.values[-1,:][0]))\n",
    "    sigma_df = pd.Series(sigma_lst, index=returns.index[-test_size:])\n",
    "    VaR_df = get_VaR_GARCH(df, alpha, sigma_df, dist)\n",
    "    return sigma_df, VaR_df\n",
    "\n",
    "PH_sigma_df_g, PH_rw_df_g = rolling_window(PH, test_size,d, alpha,p,q, vol, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041d1961",
   "metadata": {},
   "source": [
    "# Diebold-Mariano Test\n",
    "1. https://www.real-statistics.com/time-series-analysis/forecasting-accuracy/diebold-mariano-test/\n",
    "\n",
    "- Suppose that we have two forecasts f1, …, fn and g1, …, gn for a time series if y1, …, yn and we want to see which forecast is better, in the sense of it having better predictive accuracy. The obvious approach is to select the forecast that has the smaller error measurement based on one of the error measurements described in Forecasting Errors. But we need to go one step further and determine whether this difference is significant (for predictive purposes) or simply due to the specific choice of data values in the sample.\n",
    "    \n",
    "- The null hypothesis is that the two forecasts have the same accuracy. The alternative hypothesis is that the two forecasts have different levels of accuracy\n",
    "\n",
    "- Hypotheiss testing under a two tailed distribution. We use a standard normal distribution will add this to the preliminaries\n",
    "\n",
    "\n",
    "2. https://github.com/johntwk/Diebold-Mariano-Test (Reference for the code, not sure how to import the package from github kasi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdc98944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author   : John Tsang\n",
    "# Date     : December 7th, 2017\n",
    "# Purpose  : Implement the Diebold-Mariano Test (DM test) to compare \n",
    "#            forecast accuracy\n",
    "# Input    : 1) actual_lst: the list of actual values\n",
    "#            2) pred1_lst : the first list of predicted values\n",
    "#            3) pred2_lst : the second list of predicted values\n",
    "#            4) h         : the number of stpes ahead\n",
    "#            5) crit      : a string specifying the criterion \n",
    "#                             i)  MSE : the mean squared error\n",
    "#                            ii)  MAD : the mean absolute deviation\n",
    "#                           iii) MAPE : the mean absolute percentage error\n",
    "#                            iv) poly : use power function to weigh the errors\n",
    "#            6) poly      : the power for crit power \n",
    "#                           (it is only meaningful when crit is \"poly\")\n",
    "# Condition: 1) length of actual_lst, pred1_lst and pred2_lst is equal\n",
    "#            2) h must be an integer and it must be greater than 0 and less than \n",
    "#               the length of actual_lst.\n",
    "#            3) crit must take the 4 values specified in Input\n",
    "#            4) Each value of actual_lst, pred1_lst and pred2_lst must\n",
    "#               be numerical values. Missing values will not be accepted.\n",
    "#            5) power must be a numerical value.\n",
    "# Return   : a named-tuple of 2 elements\n",
    "#            1) p_value : the p-value of the DM test\n",
    "#            2) DM      : the test statistics of the DM test\n",
    "##########################################################\n",
    "# References:\n",
    "#\n",
    "# Harvey, D., Leybourne, S., & Newbold, P. (1997). Testing the equality of \n",
    "#   prediction mean squared errors. International Journal of forecasting, \n",
    "#   13(2), 281-291.\n",
    "#\n",
    "# Diebold, F. X. and Mariano, R. S. (1995), Comparing predictive accuracy, \n",
    "#   Journal of business & economic statistics 13(3), 253-264.\n",
    "#\n",
    "##########################################################\n",
    "def dm_test(actual_lst, pred1_lst, pred2_lst, h = 1, crit=\"MSE\", power = 2):\n",
    "    # Routine for checking errors\n",
    "    def error_check():\n",
    "        rt = 0\n",
    "        msg = \"\"\n",
    "        # Check if h is an integer\n",
    "        if (not isinstance(h, int)):\n",
    "            rt = -1\n",
    "            msg = \"The type of the number of steps ahead (h) is not an integer.\"\n",
    "            return (rt,msg)\n",
    "        # Check the range of h\n",
    "        if (h < 1):\n",
    "            rt = -1\n",
    "            msg = \"The number of steps ahead (h) is not large enough.\"\n",
    "            return (rt,msg)\n",
    "        len_act = len(actual_lst)\n",
    "        len_p1  = len(pred1_lst)\n",
    "        len_p2  = len(pred2_lst)\n",
    "        # Check if lengths of actual values and predicted values are equal\n",
    "        if (len_act != len_p1 or len_p1 != len_p2 or len_act != len_p2):\n",
    "            rt = -1\n",
    "            msg = \"Lengths of actual_lst, pred1_lst and pred2_lst do not match.\"\n",
    "            return (rt,msg)\n",
    "        # Check range of h\n",
    "        if (h >= len_act):\n",
    "            rt = -1\n",
    "            msg = \"The number of steps ahead is too large.\"\n",
    "            return (rt,msg)\n",
    "        # Check if criterion supported\n",
    "        if (crit != \"MSE\" and crit != \"MAPE\" and crit != \"MAD\" and crit != \"poly\"):\n",
    "            rt = -1\n",
    "            msg = \"The criterion is not supported.\"\n",
    "            return (rt,msg)  \n",
    "        # Check if every value of the input lists are numerical values\n",
    "        from re import compile as re_compile\n",
    "        comp = re_compile(\"^\\d+?\\.\\d+?$\")  \n",
    "        def compiled_regex(s):\n",
    "            \"\"\" Returns True is string is a number. \"\"\"\n",
    "            if comp.match(s) is None:\n",
    "                return s.isdigit()\n",
    "            return True\n",
    "        for actual, pred1, pred2 in zip(actual_lst, pred1_lst, pred2_lst):\n",
    "            is_actual_ok = compiled_regex(str(abs(actual)))\n",
    "            is_pred1_ok = compiled_regex(str(abs(pred1)))\n",
    "            is_pred2_ok = compiled_regex(str(abs(pred2)))\n",
    "            if (not (is_actual_ok and is_pred1_ok and is_pred2_ok)):  \n",
    "                msg = \"An element in the actual_lst, pred1_lst or pred2_lst is not numeric.\"\n",
    "                rt = -1\n",
    "                return (rt,msg)\n",
    "        return (rt,msg)\n",
    "    \n",
    "    # Error check\n",
    "    error_code = error_check()\n",
    "    # Raise error if cannot pass error check\n",
    "    if (error_code[0] == -1):\n",
    "        raise SyntaxError(error_code[1])\n",
    "        return\n",
    "    # Import libraries\n",
    "    from scipy.stats import t\n",
    "    import collections\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    # Initialise lists\n",
    "    e1_lst = []\n",
    "    e2_lst = []\n",
    "    d_lst  = []\n",
    "    \n",
    "    # convert every value of the lists into real values\n",
    "    actual_lst = pd.Series(actual_lst).apply(lambda x: float(x)).tolist()\n",
    "    pred1_lst = pd.Series(pred1_lst).apply(lambda x: float(x)).tolist()\n",
    "    pred2_lst = pd.Series(pred2_lst).apply(lambda x: float(x)).tolist()\n",
    "    \n",
    "    # Length of lists (as real numbers)\n",
    "    T = float(len(actual_lst))\n",
    "    \n",
    "    # construct d according to crit\n",
    "    if (crit == \"MSE\"):\n",
    "        for actual,p1,p2 in zip(actual_lst,pred1_lst,pred2_lst):\n",
    "            e1_lst.append((actual - p1)**2)\n",
    "            e2_lst.append((actual - p2)**2)\n",
    "        for e1, e2 in zip(e1_lst, e2_lst):\n",
    "            d_lst.append(e1 - e2)\n",
    "    elif (crit == \"MAD\"):\n",
    "        for actual,p1,p2 in zip(actual_lst,pred1_lst,pred2_lst):\n",
    "            e1_lst.append(abs(actual - p1))\n",
    "            e2_lst.append(abs(actual - p2))\n",
    "        for e1, e2 in zip(e1_lst, e2_lst):\n",
    "            d_lst.append(e1 - e2)\n",
    "    elif (crit == \"MAPE\"):\n",
    "        for actual,p1,p2 in zip(actual_lst,pred1_lst,pred2_lst):\n",
    "            e1_lst.append(abs((actual - p1)/actual))\n",
    "            e2_lst.append(abs((actual - p2)/actual))\n",
    "        for e1, e2 in zip(e1_lst, e2_lst):\n",
    "            d_lst.append(e1 - e2)\n",
    "    elif (crit == \"poly\"):\n",
    "        for actual,p1,p2 in zip(actual_lst,pred1_lst,pred2_lst):\n",
    "            e1_lst.append(((actual - p1))**(power))\n",
    "            e2_lst.append(((actual - p2))**(power))\n",
    "        for e1, e2 in zip(e1_lst, e2_lst):\n",
    "            d_lst.append(e1 - e2)    \n",
    "    \n",
    "    # Mean of d        \n",
    "    mean_d = pd.Series(d_lst).mean()\n",
    "    \n",
    "    # Find autocovariance and construct DM test statistics\n",
    "    def autocovariance(Xi, N, k, Xs):\n",
    "        autoCov = 0\n",
    "        T = float(N)\n",
    "        for i in np.arange(0, N-k):\n",
    "              autoCov += ((Xi[i+k])-Xs)*(Xi[i]-Xs)\n",
    "        return (1/(T))*autoCov\n",
    "    gamma = []\n",
    "    for lag in range(0,h):\n",
    "        gamma.append(autocovariance(d_lst,len(d_lst),lag,mean_d)) # 0, 1, 2\n",
    "    V_d = (gamma[0] + 2*sum(gamma[1:]))/T\n",
    "    DM_stat=V_d**(-0.5)*mean_d\n",
    "    harvey_adj=((T+1-2*h+h*(h-1)/T)/T)**(0.5)\n",
    "    DM_stat = harvey_adj*DM_stat\n",
    "    # Find p-value\n",
    "    p_value = 2*t.cdf(-abs(DM_stat), df = T - 1)\n",
    "    \n",
    "    # Construct named tuple for return\n",
    "    dm_return = collections.namedtuple('dm_return', 'DM p_value')\n",
    "    \n",
    "    rt = dm_return(DM = DM_stat, p_value = p_value)\n",
    "    \n",
    "    return rt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7615b91",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "776af6cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def DM_test(df, model_1, model_2, test_size):\n",
    "    returns = np.array(df['Returns'])[:test_size]\n",
    "    arr_model_1 = model_1['Forecasted VaR'].to_numpy()\n",
    "    arr_model_2 = model_2.to_numpy()\n",
    "    dm = dm_test(returns, arr_model_1, arr_model_2, h = 1, crit = 'MSE')\n",
    "    return dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2233da96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dm_return(DM=-4.288983204452318, p_value=2.3025259640654284e-05)\n",
      "dm_return(DM=-4.345339701871624, p_value=1.806265876876205e-05)\n"
     ]
    }
   ],
   "source": [
    "print(DM_test(PH, PH_fw_dn, PH_fw_df_g, 365))\n",
    "print(DM_test(PH, PH_rw_dn, PH_rw_df_g, 365))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd7e457",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
